{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import math\n",
    "import copy \n",
    "import random\n",
    "from packaging import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Dict, Optional, Tuple, Union, Any, collections\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainerState, TrainingArguments\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    WEIGHTS_NAME,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_distributed_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 4\n",
    "MAX_LEN = 128\n",
    "CAN_NUM = 100\n",
    "num_of_rerank = 30\n",
    "\n",
    "# some parameters I cooked up that work reasonably well\n",
    "epochs = 1\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "global debug\n",
    "debug = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "VOCAB_SIZE = configuration.vocab_size\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.rerank_transformer = GPT2Model(config)\n",
    "        self.rerank_linear_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        labels=None,\n",
    "        is_training=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        \n",
    "        global debug\n",
    "        # make some model parameter not change during rerank (like dropout) ??????????????\n",
    "        # model.eval()\n",
    "        self.transformer.eval()\n",
    "        \n",
    "        debug['input_ids'] = input_ids\n",
    "        debug['labels'] = labels\n",
    "\n",
    "        # rerank_places = random.sample(np.arange(1, MAX_LEN-2-CAN_NUM*2).tolist(), k=num_of_rerank) #no duplicate\n",
    "        rerank_places = random.sample(np.arange(1, MAX_LEN).tolist(), k=num_of_rerank) #no duplicate\n",
    "        rerank_places = np.concatenate(([0], np.sort(rerank_places), [MAX_LEN])) #add first and last tokens to make segments\n",
    "        debug['rerank_places'] = rerank_places\n",
    "        \n",
    "        past_key_values = None\n",
    "        all_rerank_hidden_states = []\n",
    "        all_rerank_labels = []\n",
    "        \n",
    "        if not is_training:\n",
    "            all_candidate_token_ids = []\n",
    "            all_input_ids = []\n",
    "            all_prediction_ids = []\n",
    "            \n",
    "        no_rerank_logits = []\n",
    "        check_out_num = 0\n",
    "        for i in range(num_of_rerank+1):\n",
    "            #normal stage\n",
    "            segment_input_ids = input_ids[:, rerank_places[i]:rerank_places[i+1]]\n",
    "            \n",
    "            debug['segment_input_ids'] = segment_input_ids\n",
    "\n",
    "            segment_outputs = self.transformer(\n",
    "                segment_input_ids,\n",
    "                past_key_values = past_key_values\n",
    "            )\n",
    "            \n",
    "            debug['segment_outputs'] = segment_outputs\n",
    "\n",
    "            segment_hidden = segment_outputs[0]\n",
    "            past_key_values = segment_outputs[1]\n",
    "\n",
    "            #rerank stage (just for rerank places)\n",
    "            if i == num_of_rerank:\n",
    "                break\n",
    "\n",
    "            #rerank stage\n",
    "            #get logits in rerank place\n",
    "            logits_before_rerank = self.lm_head(segment_hidden[:, -1, :])\n",
    "            #get candidate token ids according to the logits\n",
    "            candidate_token_logits, candidate_token_ids = torch.topk(logits_before_rerank, CAN_NUM)\n",
    "            rerank_labels = labels[..., rerank_places[i+1]]\n",
    "\n",
    "            debug['candidate_token_ids'] = candidate_token_ids\n",
    "            debug['rerank_labels'] = rerank_labels\n",
    "            \n",
    "            #check whether or not label in candidates\n",
    "            check_labels = rerank_labels.tolist()\n",
    "            check_candidates = candidate_token_ids.tolist()\n",
    "            \n",
    "            assert len(check_labels)==len(check_candidates)\n",
    "            \n",
    "            #check whether or not label in candidates, if not we do not do rerank\n",
    "            rerank_labels = []\n",
    "            check_in_index = []\n",
    "\n",
    "            for j in range(len(check_labels)): \n",
    "                if check_labels[j] in check_candidates[j]:\n",
    "                    rerank_labels.append(check_candidates[j].index(check_labels[j]))\n",
    "                    check_in_index.append(j)\n",
    "                else:\n",
    "                    check_out_num+=1\n",
    "            rerank_labels = torch.tensor(rerank_labels, device=input_ids.device)\n",
    "\n",
    "            if rerank_labels.shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                all_rerank_labels.append(rerank_labels)\n",
    "\n",
    "            #make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "            sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=input_ids.device) * 50256\n",
    "            candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "\n",
    "            \n",
    "            #get output from gpt2\n",
    "            rerank_outputs = self.transformer(candidate_context_ids,\n",
    "                            past_key_values=past_key_values,\n",
    "                          )\n",
    "\n",
    "            #get rerank logits for candidates\n",
    "            rerank_hidden_states = rerank_outputs[0][:, 2+CAN_NUM:2+CAN_NUM*2]\n",
    "\n",
    "            all_rerank_hidden_states.append(rerank_hidden_states[check_in_index])\n",
    "            no_rerank_logits.append(candidate_token_logits[check_in_index])\n",
    "            \n",
    "            if not is_training:\n",
    "                all_candidate_token_ids.append(candidate_token_ids[check_in_index])\n",
    "                all_prediction_ids.append(input_ids[:, rerank_places[i+1]][check_in_index])\n",
    "                all_input_ids.append(input_ids[:, :rerank_places[i+1]][check_in_index])\n",
    "\n",
    "        \n",
    "#         print(\"\\n batch info:\")\n",
    "#         print(\"there are \", check_out_num/(num_of_rerank*batch_size), \"labels not in candidates\")\n",
    "    \n",
    "        if is_training:\n",
    "            #model.train()\n",
    "            self.transformer.train()\n",
    "        \n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "\n",
    "\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "\n",
    "\n",
    "            return {\"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"rerank_loss\": rerank_loss,}\n",
    "        # for evaluation, we will evaluate the model's performance on different difficult level\n",
    "        else:\n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "            \n",
    "            for i in range(len(all_input_ids)):\n",
    "                target = torch.ones(size = [all_input_ids[i].shape[0], MAX_LEN], dtype = torch.long, device=input_ids.device) * 50256\n",
    "                target[:, :all_input_ids[i].shape[1]] = all_input_ids[i]\n",
    "                all_input_ids[i] = target\n",
    "                \n",
    "            all_input_ids = torch.cat(all_input_ids, 0)\n",
    "            all_prediction_ids = torch.cat(all_prediction_ids, 0)\n",
    "            all_candidate_token_ids = torch.cat(all_candidate_token_ids, 0)\n",
    "\n",
    "            return {\"all_rerank_logits\": all_rerank_logits,\n",
    "                    \"no_rerank_logits\": no_rerank_logits,\n",
    "                    \"difficult_level\": all_rerank_labels,\n",
    "                    \"rerank_loss\": rerank_loss,\n",
    "                    \"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"all_candidate_token_ids\": all_candidate_token_ids,\n",
    "                    \"all_prediction_ids\": all_prediction_ids,\n",
    "                    \"all_input_ids\": all_input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is in \"/mnt/nfs/work1/llcao/zonghaiyao/LM/data/wikitext-2/my_train.csv\"\n",
    "train_df = pd.read_csv(\"data/wikitext-2/my_train.csv\")\n",
    "validation_df = pd.read_csv(\"data/wikitext-2/my_validation.csv\")\n",
    "test_df = pd.read_csv(\"data/wikitext-2/my_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|endoftext|> token has the id 50256\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|endoftext|> has the id 50256\n",
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT tokenizer.\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "\n",
    "\n",
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))\n",
    "\n",
    "# instantiate the model\n",
    "# model = myGPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model = myGPT2LMHeadModel.from_pretrained(\"results/baseline_wiki2/exclude_cases_label_not_in_candidates/0\", \n",
    "                                          config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = torch.nn.DataParallel(model) # Encapsulate the model\n",
    "\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "    \n",
    "        for txt in txt_list:\n",
    "            #encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            debug['encodings_dict'] = encodings_dict\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GPT2Dataset(train_df['text'], tokenizer, max_length=MAX_LEN)\n",
    "validation_dataset = GPT2Dataset(validation_df['text'], tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = GPT2Dataset(test_df['text'], tokenizer, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# For test the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 11.93 GiB total capacity; 10.61 GiB already allocated; 106.06 MiB free; 11.29 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4f8f74b2af48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_train_rerank_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_rerank_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LM/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LM/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 11.93 GiB total capacity; 10.61 GiB already allocated; 106.06 MiB free; 11.29 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_train_normal_loss = 0\n",
    "    total_train_normal_loss_in_rerank_place = 0\n",
    "    total_train_rerank_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):        \n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(  input_ids=batch,         #batch_input_ids\n",
    "                          labels=batch,            #batch_labels\n",
    "                          is_training=True,\n",
    "                       )\n",
    "\n",
    "        debug['outputs'] = outputs\n",
    "\n",
    "        normal_loss = outputs[\"normal_loss_in_rerank_place\"].mean()\n",
    "        rerank_loss = outputs[\"rerank_loss\"].mean()\n",
    "\n",
    "        loss = normal_loss + rerank_loss\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        \n",
    "        batch_normal_loss = normal_loss.item()\n",
    "        total_train_normal_loss += batch_normal_loss\n",
    "        \n",
    "        batch_rerank_loss = rerank_loss.item()\n",
    "        total_train_rerank_loss += batch_rerank_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    avg_train_normal_loss = total_train_normal_loss / len(train_dataloader)      \n",
    "    avg_train_rerank_loss = total_train_rerank_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss:\", avg_train_loss)\n",
    "    print(\"  Average training normal_loss:\", avg_train_normal_loss)\n",
    "    print(\"  Average training rerank_loss:\", avg_train_rerank_loss)\n",
    "    print(\"  Training epoch took:\", training_time)\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    total_eval_normal_loss = 0\n",
    "    total_eval_rerank_loss = 0\n",
    "    \n",
    "    all_evaluate_rerank_logits = []\n",
    "    all_evaluate_normal_logits = []\n",
    "    all_evaluate_difficult_level = []\n",
    "    all_evaluate_rerank_loss = []\n",
    "    all_evaluate_normal_loss_in_rerank_place = []\n",
    "    all_evaluate_ground_true = []\n",
    "    all_evaluate_inputs_text = []\n",
    "    all_evaluate_candidate_token = []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(  input_ids=batch,         #batch_input_ids\n",
    "                              labels=batch,            #batch_labels\n",
    "                              is_training=False,\n",
    "                           )\n",
    "          \n",
    "            normal_loss = outputs[\"normal_loss_in_rerank_place\"].mean()\n",
    "            rerank_loss = outputs[\"rerank_loss\"].mean()\n",
    "            \n",
    "            loss = normal_loss + rerank_loss\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "        \n",
    "        batch_normal_loss = normal_loss.item()\n",
    "        total_eval_normal_loss += batch_normal_loss\n",
    "        \n",
    "        batch_rerank_loss = rerank_loss.item()\n",
    "        total_eval_rerank_loss += batch_rerank_loss\n",
    "        \n",
    "        #fine-grained evaluation\n",
    "        all_evaluate_rerank_logits.extend(outputs[\"all_rerank_logits\"].tolist())\n",
    "        all_evaluate_normal_logits.extend(outputs[\"no_rerank_logits\"].tolist())\n",
    "        all_evaluate_difficult_level.extend(outputs[\"difficult_level\"].tolist())\n",
    "        all_evaluate_rerank_loss.extend(outputs[\"rerank_loss\"].tolist())\n",
    "        all_evaluate_normal_loss_in_rerank_place.extend(outputs[\"normal_loss_in_rerank_place\"].tolist())    \n",
    "        all_evaluate_ground_true.extend(tokenizer.batch_decode(outputs['all_prediction_ids'], skip_special_tokens=True))\n",
    "        all_evaluate_inputs_text.extend(tokenizer.batch_decode(outputs['all_input_ids'], skip_special_tokens=True))\n",
    "        all_evaluate_candidate_token.extend([tokenizer.batch_decode(ids) for ids in outputs['all_candidate_token_ids']])\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    avg_val_normal_loss = total_eval_normal_loss / len(validation_dataloader)          \n",
    "    avg_val_rerank_loss = total_eval_rerank_loss / len(validation_dataloader)    \n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss:\", avg_val_loss)\n",
    "    print(\"  Average Validation normal_loss:\", avg_val_normal_loss)\n",
    "    print(\"  Average Validation rerank_loss:\", avg_val_rerank_loss)\n",
    "    print(\"  Validation took:\", validation_time)\n",
    "    \n",
    "    fine_grained_evaluation = pd.DataFrame({\"rerank_logits\": all_evaluate_rerank_logits,\n",
    "                                            \"normal_logits\": all_evaluate_normal_logits,\n",
    "                                            \"ground_true_difficulty_level\": all_evaluate_difficult_level,\n",
    "                                            \"rerank_loss\": all_evaluate_rerank_loss,\n",
    "                                            \"normal_loss\": all_evaluate_normal_loss_in_rerank_place,\n",
    "                                            \"ground_true\": all_evaluate_ground_true,\n",
    "                                            \"inputs_text\": all_evaluate_inputs_text,\n",
    "                                            \"candidate_tokens\": all_evaluate_candidate_token,\n",
    "                                            })\n",
    "    \n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def cal_entropy_difficulty_level(x):\n",
    "        x = softmax(x)\n",
    "        entropy_difficulty_level = 0\n",
    "        for i in range(CAN_NUM):\n",
    "            entropy_difficulty_level -= x[i] * np.log10(x[i])\n",
    "\n",
    "        return entropy_difficulty_level\n",
    "    \n",
    "    fine_grained_evaluation['entropy_difficulty_level'] = fine_grained_evaluation['normal_logits'].apply(cal_entropy_difficulty_level)\n",
    "#     fine_grained_evaluation.to_pickle(\"results/baseline_wiki2/exclude_cases_label_not_in_candidates/0/fine_grained_evaluation.pkl\")\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time,\n",
    "            'fine_grained_evaluation': fine_grained_evaluation,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "# print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_pickle(\"results/baseline_wiki2/exclude_cases_label_not_in_candidates/0/fine_grained_evaluation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.module.save_pretrained(\"results/baseline_wiki2/exclude_cases_label_not_in_candidates/0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cases analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = None\n",
    "for batch in validation_dataloader:\n",
    "    eval_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(eval_batch[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "#48-->colour 50-->with\n",
    "predict_token_index = 50\n",
    "past_key_values=None\n",
    "#normal stage\n",
    "segment_input_ids = eval_batch[0][3][0:predict_token_index]\n",
    "labels = eval_batch[0][3][predict_token_index]\n",
    "segment_input_ids = torch.reshape(torch.cat([segment_input_ids, segment_input_ids], 0), [-1, predict_token_index])\n",
    "segment_input_ids = segment_input_ids.cuda()\n",
    "segment_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(eval_batch[0][3]), '\\n')\n",
    "print(tokenizer.decode(segment_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_outputs = model.transformer(\n",
    "    segment_input_ids,\n",
    "    past_key_values = past_key_values\n",
    ")\n",
    "\n",
    "segment_hidden = segment_outputs[0]\n",
    "past_key_values = segment_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_before_rerank = model.lm_head(segment_hidden[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(logits_before_rerank, CAN_NUM).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.softmax(torch.topk(logits_before_rerank, CAN_NUM).values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_token_ids = torch.topk(logits_before_rerank, CAN_NUM).indices\n",
    "for i in range(CAN_NUM):\n",
    "    print(tokenizer.decode(candidate_token_ids[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_labels = labels\n",
    "tokenizer.decode(rerank_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=device) * 50256\n",
    "candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "tokenizer.decode(candidate_context_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_outputs = model.transformer(candidate_context_ids,\n",
    "                past_key_values=past_key_values,\n",
    "              )\n",
    "\n",
    "rerank_hidden_states = rerank_outputs[0][:, 2+CAN_NUM:2+CAN_NUM*2]\n",
    "rerank_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rerank_linear_head(rerank_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.softmax(model.rerank_linear_head(rerank_hidden_states), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
