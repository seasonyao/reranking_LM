{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional, Tuple, Union, Any, collections\n",
    "import torch\n",
    "import copy \n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from result_statistics import result_statistics\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    WEIGHTS_NAME,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_distributed_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import math\n",
    "import copy \n",
    "import random\n",
    "from packaging import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainerState, TrainingArguments\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "western-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "champion-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "MAX_LEN = 128\n",
    "CAN_NUM = 20\n",
    "gen_sent_len = 30\n",
    "num_sent_gen = 3\n",
    "readable_context = False\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "SAVE_PATH = \"/mnt/nfs/work1/llcao/zonghaiyao/LM/\"\n",
    "\n",
    "global debug\n",
    "debug = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wicked-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki2021_GPT2Dataset(Dataset):\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rerankGPT2LMHeadModel_generation_sent(GPT2LMHeadModel):\n",
    "    def __init__(self, config, MAX_LEN=None, CAN_NUM=None, num_of_rerank=None):\n",
    "        super().__init__(config)\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.CAN_NUM = CAN_NUM\n",
    "        self.num_of_rerank = num_of_rerank\n",
    "        self.VOCAB_SIZE = config.vocab_size\n",
    "        \n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.rerank_linear_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empty-campbell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rerankGPT2LMHeadModel_generation_sent(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (rerank_linear_head): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "\n",
    "\n",
    "model = rerankGPT2LMHeadModel_generation_sent.from_pretrained(SAVE_PATH + \"results/baseline_wiki2021/exclude_cases_label_not_in_candidates_canNUM20/120000\",\n",
    "                                                                                    config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weird-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_train_dataset.pkl', 'rb') as f:\n",
    "#     train_input_ids = pickle.load(f)\n",
    "# with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_validation_dataset.pkl', 'rb') as f:\n",
    "#     validation_input_ids = pickle.load(f)\n",
    "with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_inside_validation_dataset.pkl', 'rb') as f:\n",
    "    inside_validation_input_ids = pickle.load(f)\n",
    "# train_dataset = wiki2021_GPT2Dataset(train_input_ids)\n",
    "# validation_dataset = wiki2021_GPT2Dataset(validation_input_ids)\n",
    "inside_validation_dataset = wiki2021_GPT2Dataset(inside_validation_input_ids)\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "# train_dataloader = DataLoader(\n",
    "#             train_dataset,  # The training samples.\n",
    "#             sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "#         )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "# validation_dataloader = DataLoader(\n",
    "#             validation_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )\n",
    "\n",
    "# For inside_validation the order doesn't matter, so we'll just read them sequentially.\n",
    "inside_validation_dataloader = DataLoader(\n",
    "            inside_validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(inside_validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gross-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "def top_k_logits(logits, k, filling_value=-1e10):\n",
    "    #modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * filling_value, logits)\n",
    "\n",
    "def print_sampled_sent(generated_sent, outf, print_prefix):\n",
    "    outf.write(print_prefix + ': ' + generated_sent + '\\n')\n",
    "\n",
    "global debug\n",
    "debug = {}\n",
    "def sample_seq_stage1(model, context, gen_sent_len, device, temperature=1, top_k = 20, sample=True):\n",
    "#def sample_seq(model_condition, context, insert_loc, future_emb_chosen_arr, gen_sent_len, device, temperature=1, top_k = 5, sample=True):\n",
    "    #modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py\n",
    "    global debug\n",
    "    prev = context\n",
    "    batch_size = prev.size(0)\n",
    "    output = torch.zeros((batch_size, 0), dtype=torch.long, device = device)\n",
    "    past = None\n",
    "    #sample = False\n",
    "    for i in range(gen_sent_len):\n",
    "        outputs = model.transformer(prev, past_key_values = past)\n",
    "        logits = model.lm_head(outputs.last_hidden_state)\n",
    "        past = outputs.past_key_values\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        if sample:\n",
    "            if torch.isnan(probs).sum() > 0:\n",
    "                print(past)\n",
    "                print(prev)\n",
    "                print(insert_loc)\n",
    "                print(future_emb_chosen_arr)\n",
    "                print(logits)\n",
    "                print(probs)\n",
    "                sys.exit(0)\n",
    "            prev = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, prev = torch.topk(probs, k=1, dim=-1)\n",
    "        output = torch.cat((output, prev), dim=1)\n",
    "    return output\n",
    "\n",
    "def sample_seq_stage2(model, context, gen_sent_len, device, temperature=1, top_k = 20, sample=True):\n",
    "#def sample_seq(model_condition, context, insert_loc, future_emb_chosen_arr, gen_sent_len, device, temperature=1, top_k = 5, sample=True):\n",
    "    #modified from https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/sample.py\n",
    "    global debug\n",
    "    prev = context\n",
    "    batch_size = prev.size(0)\n",
    "    output = torch.zeros((batch_size, 0), dtype=torch.long, device = device)\n",
    "    past = None\n",
    "    #sample = False\n",
    "    for i in range(gen_sent_len):\n",
    "        debug['prev'] = prev\n",
    "        debug['past'] = past\n",
    "        outputs = model.transformer(prev, past_key_values = past)\n",
    "        logits = model.lm_head(outputs.last_hidden_state)[:, -1, :]\n",
    "        past = outputs.past_key_values\n",
    "        \n",
    "        candidate_token_logits, candidate_token_ids = torch.topk(logits, top_k)\n",
    "        #make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "        sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=prev.device) * 50256\n",
    "        candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "        rerank_outputs = model.transformer(candidate_context_ids,\n",
    "                                          past_key_values=past\n",
    "                                          )\n",
    "        rerank_logits = model.rerank_linear_head(rerank_outputs.last_hidden_state[:, 2+top_k:2+top_k*2])  \n",
    "        probs = F.softmax(rerank_logits, dim=1).reshape([-1, top_k])\n",
    "        if sample:\n",
    "            if torch.isnan(probs).sum() > 0:\n",
    "                print(past)\n",
    "                print(prev)\n",
    "                print(insert_loc)\n",
    "                print(future_emb_chosen_arr)\n",
    "                print(logits)\n",
    "                print(probs)\n",
    "                sys.exit(0)\n",
    "            \n",
    "            tmp_prev = torch.multinomial(probs, num_samples=1).tolist()\n",
    "            prev = []\n",
    "            for i in range(probs.shape[0]):\n",
    "                prev.append([candidate_token_ids[i][tmp_prev[i][0]].tolist()])\n",
    "            prev = torch.tensor(prev, device=probs.device)\n",
    "#         else:\n",
    "#             _, prev = torch.topk(probs, k=1, dim=-1)\n",
    "        output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exclusive-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# # Evaluate data for one epoch\n",
    "# for batch in inside_validation_dataloader:   \n",
    "#     batch = batch.to(device)\n",
    "#     with torch.no_grad():  \n",
    "        \n",
    "#         output = sample_seq_stage2(model=model,\n",
    "#                                    context=batch[:, :20], \n",
    "#                                    gen_sent_len=30,\n",
    "#                                    device=device,\n",
    "#                                    temperature=1,\n",
    "#                                    top_k = 20, \n",
    "#                                    sample=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-audio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-nancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "color-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_context(context, outf = None, shortest_context = 50, longest_context = 150):\n",
    "    bad_context = False\n",
    "    context = context.replace('â',\"'\").replace('â','-').replace('\\n',\" \")\n",
    "    if len(context.split()) < shortest_context:\n",
    "    #if len(context.split()) < 5:\n",
    "        if outf is not None:\n",
    "            outf.write(\"Skip due to short context\\n\")\n",
    "        bad_context = True\n",
    "    #if len(context.split()) > 50:\n",
    "    if len(context.split()) > longest_context:\n",
    "        if outf is not None:\n",
    "            outf.write(\"Skip due to long context\\n\")\n",
    "        bad_context = True\n",
    "    try:\n",
    "        context.encode('ascii', 'strict')\n",
    "    except:\n",
    "        if outf is not None:\n",
    "            outf.write(\"Skip due to special token\\n\")\n",
    "        bad_context = True\n",
    "    return context, bad_context\n",
    "\n",
    "def get_word_list_spacy(inner_idx_tensor, feature_text, tokenizer_GPT2, nlp):\n",
    "    def get_word_list_from_text(feature_text_i):\n",
    "        feature_text_i_str = tokenizer_GPT2.convert_tokens_to_string(feature_text_i)\n",
    "        tokens = nlp.tokenizer(feature_text_i_str)\n",
    "        word_raw_list_i_j = []\n",
    "\n",
    "        for tok in tokens:\n",
    "            w = tok.text\n",
    "            word_raw_list_i_j.append(w)\n",
    "            \n",
    "        return word_raw_list_i_j\n",
    "    word_raw_list = []\n",
    "    word_raw_rest_list = []\n",
    "    batch_size, num_head = inner_idx_tensor.size()\n",
    "    inner_idx_tensor_np = inner_idx_tensor.cpu().numpy()\n",
    "    for b, feature_text_i in enumerate(feature_text):\n",
    "        word_raw_list_i = []\n",
    "        word_raw_rest_list_i = []\n",
    "        for j in range(num_head):\n",
    "            end_idx = inner_idx_tensor_np[b,j]\n",
    "            word_raw_list_i_j = get_word_list_from_text(feature_text_i[:end_idx])\n",
    "            #assert len(word_idx_list_i_j) > 0, print(feature_text_i[:end_idx])\n",
    "            word_raw_list_i.append(word_raw_list_i_j)\n",
    "            if end_idx == len(feature_text_i):\n",
    "                word_raw_rest_list_i.append([])\n",
    "            else:\n",
    "                word_raw_rest_list_i_j = get_word_list_from_text(feature_text_i[end_idx:])\n",
    "                word_raw_rest_list_i.append(word_raw_rest_list_i_j)\n",
    "            #count = word_idx_d2_count.get(w_idx,0)\n",
    "            #word_idx_d2_count[w_idx] += 1\n",
    "        word_raw_list.append(word_raw_list_i)\n",
    "        word_raw_rest_list.append(word_raw_rest_list_i)\n",
    "    return word_raw_list, word_raw_rest_list\n",
    "\n",
    "def print_eval_text(feature, i_batch, outf, tokenizer_GPT2, inner_idx_tensor, gen_sent_tensor, gen_sent_tensor_org, result_stats, word_raw_list, word_raw_rest_list, readable_context, run_eval):\n",
    "    #batch_size, num_head, top_k, n_basis = top_index.size()\n",
    "    #num_sent_gen = gen_sent_tensor.size(2)\n",
    "    batch_size, num_head, num_sent_gen, gen_sent_len = gen_sent_tensor.size()\n",
    "    \n",
    "    #not_ascii_multi = 0\n",
    "    #not_ascii_org = 0\n",
    "    for i_sent in range(batch_size):\n",
    "        outf.write('batch number: ' + str(i_sent) + '\\n')\n",
    "        last_end = -1\n",
    "        # for m in range(1):\n",
    "        for m in range(num_head):\n",
    "            outf.write('number of head: ' + str(m) + '\\n')\n",
    "            end = inner_idx_tensor[i_sent,m].item()\n",
    "            if end == last_end:\n",
    "                continue\n",
    "            last_end = end\n",
    "            \n",
    "            #outf.write(tokenizer_GPT2.convert_tokens_to_string(feature_text[i_sent][:end])+'\\n')\n",
    "            context = tokenizer_GPT2.decode(feature[i_sent,:end])\n",
    "            \n",
    "            if readable_context:\n",
    "                context, bad_context = preprocessing_context(context, outf)\n",
    "                if bad_context:\n",
    "                    continue\n",
    "\n",
    "            outf.write(context+'\\n')\n",
    "            outf.write('\\n')\n",
    "            \n",
    "            multi_sent_list = []\n",
    "            org_sent_list = []\n",
    "            for j in range(num_sent_gen):\n",
    "#                 print(gen_sent_tensor)\n",
    "#                 generated_sent = tokenizer_GPT2.decode(gen_sent_tensor[i_sent, m, j, :])\n",
    "#                 multi_sent_list.append(generated_sent)\n",
    "                if gen_sent_tensor_org.size(0) > 0:\n",
    "                    generated_sent_org = tokenizer_GPT2.decode( gen_sent_tensor_org[i_sent, m, j, :] )\n",
    "                    org_sent_list.append(generated_sent_org)\n",
    "\n",
    "#             for j in range(num_sent_gen):\n",
    "#                 generated_sent = multi_sent_list[j]\n",
    "#                 print_sampled_sent(generated_sent, outf, 'multi-facet '+ str(j))\n",
    "#                 if run_eval:\n",
    "#                     result_stats.update(\"Multi-facet\", gen_sent_tensor[i_sent, m, j, :], feature[i_sent,:end], tokenizer_GPT2, word_raw_list[i_sent][m], word_raw_rest_list[i_sent][m], m)\n",
    "#             if run_eval:\n",
    "#                 result_stats.update_self_BLEU(\"Multi-facet\", m)\n",
    "            if gen_sent_tensor_org.size(0) > 0:\n",
    "                for j in range(num_sent_gen):\n",
    "                    generated_sent_org = org_sent_list[j]\n",
    "                    print_sampled_sent(generated_sent_org, outf, 'single-facet '+ str(j))\n",
    "                    if run_eval:\n",
    "                        result_stats.update(\"Single-facet\", gen_sent_tensor_org[i_sent, m, j, :], feature[i_sent,:end], tokenizer_GPT2, word_raw_list[i_sent][m], word_raw_rest_list[i_sent][m], m)\n",
    "                if run_eval:\n",
    "                    result_stats.update_self_BLEU(\"Single-facet\", m)\n",
    "\n",
    "            if run_eval:\n",
    "                result_stats.renew_ngram(m)\n",
    "    #outf.write('Number of not ascii code in multi: {}, in single: {}: {}\\n'.format(not_ascii_conditional, not_ascii_org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "going-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_interactive_LM(model_multi, model_single, gpt2_model, device, num_sent_gen, gen_sent_len, dataloader, outf, max_batch_num, tokenizer_GPT2, bptt, readable_context = False, run_eval = True):\n",
    "    top_k = 5\n",
    "    nlp = English()\n",
    "\n",
    "    #emb_sum = torch.sum(word_norm_emb,dim=1)\n",
    "    #OOV_list = torch.nonzero(emb_sum == 0).squeeze().cpu().tolist()\n",
    "    #print(\"OOV number = {}\".format(len(OOV_list)))\n",
    "    #print(\"OOV index examples {}\".format(OOV_list[:10]))\n",
    "    #OOV_set = set(OOV_list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if run_eval:\n",
    "            result_stats = result_statistics(gpt2_model)\n",
    "#             result_stats.add_model(\"Multi-facet\")\n",
    "            result_stats.add_model(\"Single-facet\")\n",
    "        else:\n",
    "            result_stats = []\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            sample_batched = sample_batched.to(device)\n",
    "            # if i_batch == 0:\n",
    "            #     continue\n",
    "            print(\"batch\"+str(i_batch))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            feature = sample_batched\n",
    "            \n",
    "            feature_text = [ [tokenizer_GPT2._convert_id_to_token(x) for x in feature[i,:].tolist()] for i in range(feature.size(0))]\n",
    "\n",
    "            max_prompt_len = bptt - gen_sent_len\n",
    "            min_prompt_len = 20\n",
    "            interval = gen_sent_len\n",
    "            \n",
    "            batch_size = feature.size(0)\n",
    "            start_idx_list = list(range(min_prompt_len,max_prompt_len,interval))\n",
    "            num_head = len(start_idx_list)\n",
    "            inner_idx_tensor = torch.tensor(start_idx_list, dtype=torch.long, device = device)\n",
    "            inner_idx_tensor = inner_idx_tensor.expand(batch_size, num_head)\n",
    "            batch_size, num_head = inner_idx_tensor.size()\n",
    "\n",
    "            gen_sent_tensor = torch.empty( (batch_size, num_head, num_sent_gen, gen_sent_len), dtype=torch.long, device=device )\n",
    "            gen_sent_tensor_org = torch.empty( (batch_size, num_head, num_sent_gen, gen_sent_len), dtype=torch.long, device=device )\n",
    "\n",
    "            word_raw_list, word_raw_rest_list = get_word_list_spacy(inner_idx_tensor, feature_text, tokenizer_GPT2, nlp)\n",
    "            # for i_sent in range(1):\n",
    "            for i_sent in range(batch_size):\n",
    "                print(\"sent\"+str(i_sent))\n",
    "                last_end = -1\n",
    "                \n",
    "                # for m in range(1):\n",
    "                for m in range(num_head):\n",
    "                    print(\"head\"+str(m))\n",
    "\n",
    "                    end = inner_idx_tensor[i_sent,m]\n",
    "                    if end == last_end:\n",
    "                        continue\n",
    "                    last_end = end\n",
    "                    \n",
    "                    context = tokenizer_GPT2.convert_tokens_to_string(feature_text[i_sent][:end])\n",
    "                    if readable_context:\n",
    "                        context_proc, bad_context = preprocessing_context(context, outf)\n",
    "                        if bad_context:\n",
    "                            continue\n",
    "\n",
    "                    end_int = end.item()\n",
    "                    \n",
    "                    start_int = 0\n",
    "                    #if end_int > max_prompt_len:\n",
    "                    #    start_int = end_int - max_prompt_len\n",
    "                    \n",
    "#                     t = time.time()\n",
    "                    feature_expanded = feature[i_sent,start_int:end].unsqueeze(0).expand(num_sent_gen,end_int - start_int).to(device = device)\n",
    "#                     if model_multi.output_probs:\n",
    "#                         output = sample_seq_prob(model_multi, feature_expanded, gen_sent_len, device)\n",
    "#                     else:\n",
    "#                         output = sample_seq(model_multi, feature_expanded, gen_sent_len, device)\n",
    "#                     multi_elapsed = time.time() - t\n",
    "#                     gen_sent_tensor[i_sent, m, :, :] = output\n",
    "                    \n",
    "                    t = time.time()\n",
    "#                     if model_single.output_probs:\n",
    "#                         output_org = sample_seq_prob(model_single, feature_expanded, gen_sent_len, device)\n",
    "#                     else:\n",
    "                    output_org = sample_seq_stage2(model_single, feature_expanded, gen_sent_len, device)\n",
    "                    org_elapsed = time.time() - t\n",
    "                    gen_sent_tensor_org[i_sent, m, :, :] = output_org\n",
    "                    \n",
    "                    \n",
    "#                     if run_eval:\n",
    "#                         #result_stats.model_results[\"time_count\"] += 1\n",
    "#                         for method_name, time_spent in [ (\"Multi-facet\",multi_elapsed), (\"Single-facet\", org_elapsed)]:\n",
    "#                             result_stats.model_results[method_name][\"time_sum\"] += time_spent\n",
    "#                             result_stats.model_results[method_name][\"time_count\"] += 1\n",
    "\n",
    "                \n",
    "            print_eval_text(feature, i_batch, outf, tokenizer_GPT2, inner_idx_tensor, gen_sent_tensor, gen_sent_tensor_org, result_stats, word_raw_list, word_raw_rest_list,  readable_context, run_eval)\n",
    "            if i_batch + 1 >= max_batch_num:\n",
    "                break\n",
    "        if run_eval:\n",
    "            result_stats.print()\n",
    "            result_stats.generate_report(outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affiliated-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_org_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "outf = open('generated_stage2.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "boring-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "falling-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_interactive_LM(model_multi=None, model_single=model, gpt2_model=gpt2_org_model, \n",
    "                         device=device, num_sent_gen=3, gen_sent_len=30, dataloader=inside_validation_dataloader,\n",
    "                         outf=outf, max_batch_num=300, tokenizer_GPT2=tokenizer_GPT2, bptt=MAX_LEN, \n",
    "                         readable_context = False, run_eval = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "criminal-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:23:32'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_time(time.time() - t0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "angry-algebra",
   "metadata": {},
   "source": [
    "stage1:\n",
    "time:'1:00:26'\n",
    "Single-facet count:  10800\n",
    "Single-facet BLEU count:  10737\n",
    "Single-facet BLEU:  0.08085207428097457\n",
    "Single-facet BLEU_arr: [0.07641832219981144, 0.08171892386531732, 0.08446868441864999]\n",
    "Single-facet context_len_arr: [16.998331943286072, 42.80534670008354, 68.83431952662721]\n",
    "Single-facet self BLEU:  0.12315137804694977\n",
    "Single-facet self BLEU arr: [0.11827476821171226, 0.12331287520174587, 0.12793053535731874]\n",
    "Single-facet context BLEU:  0.07218738868980977\n",
    "Single-facet BLEU Diff: 0.008664685591164803\n",
    "Single-facet perplexity:  30.021781842581596\n",
    "Single-facet unigram:  0.6147627911751979\n",
    "Single-facet bigram:  0.9037576892909903\n",
    "Single-facet unigram arr: [0.6112642629559576, 0.6195263928966149, 0.6134977176730239]\n",
    "Single-facet bigram arr: [0.9028437679343013, 0.9056798904644884, 0.9027494094741748]\n",
    "\n",
    "stage2:\n",
    "time:'1:23:32'\n",
    "Single-facet count:  10800\n",
    "Single-facet BLEU count:  10737\n",
    "Single-facet BLEU:  0.08316475101568013\n",
    "Single-facet BLEU_arr: [0.07759600889680318, 0.08439072727621218, 0.08756832516334774]\n",
    "Single-facet context_len_arr: [16.998331943286072, 42.80534670008354, 68.83431952662721]\n",
    "Single-facet self BLEU:  0.1285293881837052\n",
    "Single-facet self BLEU arr: [0.12225130239100963, 0.12992900822850112, 0.1334762010931117]\n",
    "Single-facet context BLEU:  0.08305997263402502\n",
    "Single-facet BLEU Diff: 0.00010477838165511011\n",
    "Single-facet perplexity:  25.111633203190696\n",
    "Single-facet unigram:  0.6085615433417686\n",
    "Single-facet bigram:  0.901731755766543\n",
    "Single-facet unigram arr: [0.6088357296019252, 0.6082473021272925, 0.6086015982960893]\n",
    "Single-facet bigram arr: [0.9046390185608597, 0.89962758366665, 0.9009286650721134]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
