{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional, Tuple, Union, Any, collections\n",
    "import torch\n",
    "import copy \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    WEIGHTS_NAME,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_distributed_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import math\n",
    "import copy \n",
    "import random\n",
    "from packaging import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainerState, TrainingArguments\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "champion-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "MAX_LEN = 128\n",
    "CAN_NUM = 20\n",
    "num_of_rerank = 30\n",
    "\n",
    "# some parameters I cooked up that work reasonably well\n",
    "epochs = 1\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "SAVE_PATH = \"/mnt/nfs/work1/llcao/zonghaiyao/LM/\"\n",
    "\n",
    "global debug\n",
    "debug = {}\n",
    "debug['check_in_num'] = 0\n",
    "debug['total_sample_num'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wicked-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki2021_GPT2Dataset(Dataset):\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates(GPT2LMHeadModel):\n",
    "    def __init__(self, config, MAX_LEN, CAN_NUM, num_of_rerank):\n",
    "        super().__init__(config)\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.CAN_NUM = CAN_NUM\n",
    "        self.num_of_rerank = num_of_rerank\n",
    "        self.VOCAB_SIZE = config.vocab_size\n",
    "        \n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.rerank_transformer = GPT2Model(config)\n",
    "        self.rerank_linear_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        labels=None,\n",
    "        is_training=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        # make some model parameter not change during rerank (like dropout) ??????????????\n",
    "        # model.eval()\n",
    "        global debug\n",
    "        if is_training:\n",
    "            self.transformer.eval()\n",
    "\n",
    "        rerank_places = random.sample(np.arange(1, self.MAX_LEN).tolist(), k=self.num_of_rerank) #no duplicate\n",
    "        rerank_places = np.concatenate(([0], np.sort(rerank_places), [self.MAX_LEN])) #add first and last tokens to make segments\n",
    "        \n",
    "        past_key_values = None\n",
    "        all_rerank_hidden_states = []\n",
    "        all_rerank_labels = []\n",
    "        no_rerank_logits = []\n",
    "        check_out_num = 0\n",
    "        \n",
    "        if not is_training:\n",
    "            all_candidate_token_ids = []\n",
    "            all_input_ids = []\n",
    "            all_prediction_ids = []\n",
    "        \n",
    "        for i in range(self.num_of_rerank+1):\n",
    "            #normal stage\n",
    "            segment_input_ids = input_ids[:, rerank_places[i]:rerank_places[i+1]]\n",
    "\n",
    "            segment_outputs = self.transformer(\n",
    "                segment_input_ids,\n",
    "                past_key_values = past_key_values\n",
    "            )\n",
    "\n",
    "            segment_hidden = segment_outputs[0]\n",
    "            past_key_values = segment_outputs[1]\n",
    "\n",
    "            #rerank stage (just for rerank places)\n",
    "            if i == self.num_of_rerank:\n",
    "                break\n",
    "\n",
    "            #rerank stage\n",
    "            #get logits in rerank place\n",
    "            logits_before_rerank = self.lm_head(segment_hidden[:, -1, :])\n",
    "            #get candidate token ids according to the logits\n",
    "            candidate_token_logits, candidate_token_ids = torch.topk(logits_before_rerank, self.CAN_NUM)\n",
    "            rerank_labels = labels[..., rerank_places[i+1]]\n",
    "            \n",
    "            debug['logits_before_rerank'] = logits_before_rerank\n",
    "            debug['candidate_token_ids'] = candidate_token_ids\n",
    "            debug['rerank_labels'] = rerank_labels\n",
    "            \n",
    "            assert 1==0\n",
    "            \n",
    "            #check whether or not label in candidates\n",
    "            check_labels = rerank_labels.tolist()\n",
    "            check_candidates = candidate_token_ids.tolist()\n",
    "            \n",
    "            assert len(check_labels)==len(check_candidates)\n",
    "            \n",
    "            #check whether or not label in candidates, if not we do not do rerank\n",
    "            rerank_labels = []\n",
    "            check_in_index = []\n",
    "\n",
    "            for j in range(len(check_labels)): \n",
    "                if check_labels[j] in check_candidates[j]:\n",
    "                    rerank_labels.append(check_candidates[j].index(check_labels[j]))\n",
    "                    check_in_index.append(j)\n",
    "                else:\n",
    "                    check_out_num+=1\n",
    "            rerank_labels = torch.tensor(rerank_labels, device=input_ids.device)\n",
    "\n",
    "            if rerank_labels.shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                all_rerank_labels.append(rerank_labels)\n",
    "\n",
    "            #make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "            sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=input_ids.device) * 50256\n",
    "            candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "\n",
    "            \n",
    "            #get output from gpt2\n",
    "            rerank_outputs = self.transformer(candidate_context_ids,\n",
    "                            past_key_values=past_key_values,\n",
    "                          )\n",
    "\n",
    "            #get rerank logits for candidates\n",
    "            rerank_hidden_states = rerank_outputs[0][:, 2+self.CAN_NUM:2+self.CAN_NUM*2]\n",
    "\n",
    "            all_rerank_hidden_states.append(rerank_hidden_states[check_in_index])\n",
    "            no_rerank_logits.append(candidate_token_logits[check_in_index])\n",
    "            \n",
    "            if not is_training:\n",
    "                all_candidate_token_ids.append(candidate_token_ids[check_in_index])\n",
    "                all_prediction_ids.append(input_ids[:, rerank_places[i+1]][check_in_index])\n",
    "                all_input_ids.append(input_ids[:, :rerank_places[i+1]][check_in_index])\n",
    "\n",
    "        \n",
    "#         print(\"\\n batch info:\")\n",
    "#         print(\"there are \", check_out_num/(self.num_of_rerank*batch_size), \"labels not in candidates\")\n",
    "    \n",
    "        if is_training:\n",
    "            #model.train()\n",
    "            self.transformer.train()\n",
    "        \n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, self.CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "\n",
    "\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, self.CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "\n",
    "\n",
    "            return {\"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"rerank_loss\": rerank_loss,}\n",
    "        # for evaluation, we will evaluate the model's performance on different difficult level\n",
    "        else:\n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, self.CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, self.CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "            \n",
    "            for i in range(len(all_input_ids)):\n",
    "                target = torch.ones(size = [all_input_ids[i].shape[0], self.MAX_LEN], dtype = torch.long, device=input_ids.device) * 50256\n",
    "                target[:, :all_input_ids[i].shape[1]] = all_input_ids[i]\n",
    "                all_input_ids[i] = target\n",
    "                \n",
    "            all_input_ids = torch.cat(all_input_ids, 0)\n",
    "            all_prediction_ids = torch.cat(all_prediction_ids, 0)\n",
    "            all_candidate_token_ids = torch.cat(all_candidate_token_ids, 0)\n",
    "\n",
    "            return {\"all_rerank_logits\": all_rerank_logits,\n",
    "                    \"no_rerank_logits\": no_rerank_logits,\n",
    "                    \"difficult_level\": all_rerank_labels,\n",
    "                    \"rerank_loss\": rerank_loss,\n",
    "                    \"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"all_candidate_token_ids\": all_candidate_token_ids,\n",
    "                    \"all_prediction_ids\": all_prediction_ids,\n",
    "                    \"all_input_ids\": all_input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empty-campbell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates were not initialized from the model checkpoint at results/baseline_wiki2021/not_stage2_canNUM20/last_model and are newly initialized: ['rerank_linear_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    (rerank_linear_head): Linear(in_features=768, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "\n",
    "# instantiate the model\n",
    "# model = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\"results/baseline_wiki2021/exclude_cases_label_not_in_candidates_canNUM20/10000\", \n",
    "#                                                                                     config=configuration,\n",
    "#                                                                                     MAX_LEN = MAX_LEN,\n",
    "#                                                                                     CAN_NUM = CAN_NUM, \n",
    "#                                                                                     num_of_rerank = num_of_rerank)\n",
    "# model = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\"gpt2\", \n",
    "#                                                                                     config=configuration,\n",
    "#                                                                                     MAX_LEN = MAX_LEN,\n",
    "#                                                                                     CAN_NUM = CAN_NUM, \n",
    "#                                                                                     num_of_rerank = num_of_rerank)\n",
    "model = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\"results/baseline_wiki2021/not_stage2_canNUM20/last_model\", \n",
    "                                                                                    config=configuration,\n",
    "                                                                                    MAX_LEN = MAX_LEN,\n",
    "                                                                                    CAN_NUM = CAN_NUM, \n",
    "                                                                                    num_of_rerank = num_of_rerank)\n",
    "\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = torch.nn.DataParallel(model) # Encapsulate the model\n",
    "\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_train_dataset.pkl', 'rb') as f:\n",
    "#     train_input_ids = pickle.load(f)\n",
    "with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_validation_dataset.pkl', 'rb') as f:\n",
    "    validation_input_ids = pickle.load(f)\n",
    "with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_inside_validation_dataset.pkl', 'rb') as f:\n",
    "    inside_validation_input_ids = pickle.load(f)\n",
    "    \n",
    "# train_dataset = wiki2021_GPT2Dataset(train_input_ids)\n",
    "validation_dataset = wiki2021_GPT2Dataset(validation_input_ids)\n",
    "inside_validation_dataset = wiki2021_GPT2Dataset(inside_validation_input_ids)\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "# train_dataloader = DataLoader(\n",
    "#             train_dataset,  # The training samples.\n",
    "#             sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "#         )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# For inside_validation the order doesn't matter, so we'll just read them sequentially.\n",
    "inside_validation_dataloader = DataLoader(\n",
    "            inside_validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(inside_validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gross-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exclusive-journalism",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bbb531288ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minside_validation_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         outputs = model(input_ids=batch,         #batch_input_ids\n\u001b[0m\u001b[1;32m     24\u001b[0m                         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;31m#batch_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LM/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LM/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LM/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-190fa7d93dc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, labels, is_training)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mdebug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rerank_labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#check whether or not label in candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_eval_loss = 0\n",
    "total_eval_normal_loss = 0\n",
    "total_eval_rerank_loss = 0\n",
    "\n",
    "all_evaluate_rerank_logits = []\n",
    "all_evaluate_normal_logits = []\n",
    "all_evaluate_difficult_level = []\n",
    "all_evaluate_rerank_loss = []\n",
    "all_evaluate_normal_loss_in_rerank_place = []\n",
    "all_evaluate_ground_true = []\n",
    "all_evaluate_inputs_text = []\n",
    "all_evaluate_candidate_token = []\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in inside_validation_dataloader:        \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(input_ids=batch,         #batch_input_ids\n",
    "                        labels=batch,            #batch_labels\n",
    "                        is_training=False,\n",
    "                        )\n",
    "\n",
    "        normal_loss = outputs[\"normal_loss_in_rerank_place\"].mean()\n",
    "        rerank_loss = outputs[\"rerank_loss\"].mean()\n",
    "\n",
    "        loss = normal_loss + rerank_loss\n",
    "\n",
    "    batch_loss = loss.item()\n",
    "    total_eval_loss += batch_loss        \n",
    "\n",
    "    batch_normal_loss = normal_loss.item()\n",
    "    total_eval_normal_loss += batch_normal_loss\n",
    "\n",
    "    batch_rerank_loss = rerank_loss.item()\n",
    "    total_eval_rerank_loss += batch_rerank_loss\n",
    "\n",
    "    #fine-grained evaluation\n",
    "    all_evaluate_rerank_logits.extend(outputs[\"all_rerank_logits\"].tolist())\n",
    "    all_evaluate_normal_logits.extend(outputs[\"no_rerank_logits\"].tolist())\n",
    "    all_evaluate_difficult_level.extend(outputs[\"difficult_level\"].tolist())\n",
    "    all_evaluate_rerank_loss.extend(outputs[\"rerank_loss\"].tolist())\n",
    "    all_evaluate_normal_loss_in_rerank_place.extend(outputs[\"normal_loss_in_rerank_place\"].tolist())    \n",
    "    all_evaluate_ground_true.extend(tokenizer.batch_decode(outputs['all_prediction_ids'], skip_special_tokens=True))\n",
    "    all_evaluate_inputs_text.extend(tokenizer.batch_decode(outputs['all_input_ids'], skip_special_tokens=True))\n",
    "    all_evaluate_candidate_token.extend([tokenizer.batch_decode(ids) for ids in outputs['all_candidate_token_ids']])\n",
    "\n",
    "\n",
    "avg_val_loss = total_eval_loss / len(inside_validation_dataloader)\n",
    "avg_val_normal_loss = total_eval_normal_loss / len(inside_validation_dataloader)       \n",
    "avg_val_rerank_loss = total_eval_rerank_loss / len(inside_validation_dataloader)    \n",
    "\n",
    "validation_time = format_time(time.time() - t1)    \n",
    "\n",
    "print(\"  inside Validation Loss:\", avg_val_loss)\n",
    "print(\"  Average inside Validation normal_loss:\", avg_val_normal_loss)\n",
    "print(\"  Average inside Validation rerank_loss:\", avg_val_rerank_loss)\n",
    "print(\"  inside Validation took:\", validation_time)\n",
    "\n",
    "#fine_grained_evaluation\n",
    "fg_eval = pd.DataFrame({\"rerank_logits\": all_evaluate_rerank_logits,\n",
    "                        \"normal_logits\": all_evaluate_normal_logits,\n",
    "                        \"ground_true_difficulty_level\": all_evaluate_difficult_level,\n",
    "                        \"rerank_loss\": all_evaluate_rerank_loss,\n",
    "                        \"normal_loss\": all_evaluate_normal_loss_in_rerank_place,\n",
    "                        \"ground_true\": all_evaluate_ground_true,\n",
    "                        \"inputs_text\": all_evaluate_inputs_text,\n",
    "                        \"candidate_tokens\": all_evaluate_candidate_token,\n",
    "                        }) \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def cal_entropy_difficulty_level(x):\n",
    "    x = softmax(x)\n",
    "    entropy_difficulty_level = 0\n",
    "    for i in range(CAN_NUM):\n",
    "        entropy_difficulty_level -= x[i] * np.log10(x[i])\n",
    "\n",
    "    return entropy_difficulty_level\n",
    "\n",
    "fg_eval['entropy_difficulty_level'] = fg_eval['normal_logits'].apply(cal_entropy_difficulty_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fg_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bacterial-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_token_logits, candidate_token_ids = torch.topk(debug['logits_before_rerank'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "creative-chapel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.7717, 5.7697, 5.7257,  ..., 2.1319, 2.0612, 2.0487],\n",
       "        [7.9025, 7.7173, 7.3267,  ..., 3.0679, 3.0306, 3.0210],\n",
       "        [6.7422, 5.1647, 5.0419,  ..., 3.2128, 3.1623, 3.1347],\n",
       "        ...,\n",
       "        [6.7209, 5.7657, 4.9293,  ..., 3.4125, 3.4082, 3.3533],\n",
       "        [8.7585, 7.9174, 7.8672,  ..., 4.3438, 4.3235, 4.2999],\n",
       "        [6.2602, 5.3347, 4.7086,  ..., 3.5911, 3.5759, 3.5080]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cleared-insider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,    11,   287,  ...,  5140,   326,    26],\n",
       "        [  287,   788,   284,  ...,   351,   416,   276],\n",
       "        [  373,   318,    11,  ..., 25409,   546, 16199],\n",
       "        ...,\n",
       "        [   13,    11,   290,  ...,  1120,   475,   940],\n",
       "        [ 4708,  4346, 44185,  ...,  8674, 50196, 15581],\n",
       "        [ 2888,  1255,   636,  ..., 21018, 24505,  1862]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adult-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_token_logits, candidate_token_ids = torch.sort(debug['logits_before_rerank'], descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smoking-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7717,  5.7697,  5.7257,  ..., -4.7486, -4.8286, -4.9045],\n",
       "        [ 7.9025,  7.7173,  7.3267,  ..., -7.5401, -7.7853, -8.0396],\n",
       "        [ 6.7422,  5.1647,  5.0419,  ..., -4.5140, -4.5846, -4.6381],\n",
       "        ...,\n",
       "        [ 6.7209,  5.7657,  4.9293,  ..., -5.6367, -5.8668, -5.8695],\n",
       "        [ 8.7585,  7.9174,  7.8672,  ..., -5.9153, -5.9355, -6.1981],\n",
       "        [ 6.2602,  5.3347,  4.7086,  ..., -3.6288, -3.6861, -3.8877]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "artistic-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,    11,   287,  ..., 29157, 43816, 27596],\n",
       "        [  287,   788,   284,  ...,  2134,   675,  7100],\n",
       "        [  373,   318,    11,  ..., 31814, 26277, 35044],\n",
       "        ...,\n",
       "        [   13,    11,   290,  ...,  4985, 30152, 24957],\n",
       "        [ 4708,  4346, 44185,  ..., 17511,  3976,  5828],\n",
       "        [ 2888,  1255,   636,  ..., 47252,  9868,  7732]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dental-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9289e-02, 2.9233e-02, 2.7973e-02,  ..., 7.9037e-07, 7.2960e-07,\n",
       "         6.7625e-07],\n",
       "        [1.4620e-01, 1.2148e-01, 8.2196e-02,  ..., 2.8729e-08, 2.2480e-08,\n",
       "         1.7433e-08],\n",
       "        [2.9327e-02, 6.0556e-03, 5.3557e-03,  ..., 3.7909e-07, 3.5326e-07,\n",
       "         3.3486e-07],\n",
       "        ...,\n",
       "        [3.0249e-02, 1.1639e-02, 5.0426e-03,  ..., 1.2998e-07, 1.0326e-07,\n",
       "         1.0298e-07],\n",
       "        [9.9348e-02, 4.2845e-02, 4.0746e-02,  ..., 4.2112e-08, 4.1272e-08,\n",
       "         3.1738e-08],\n",
       "        [8.6561e-03, 3.4308e-03, 1.8342e-03,  ..., 4.3914e-07, 4.1466e-07,\n",
       "         3.3897e-07]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.nn.functional.softmax(candidate_token_logits, dim=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opened-detective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0293, 0.0585, 0.0865,  ..., 0.1308, 0.1315, 0.1322],\n",
       "        [0.1462, 0.2677, 0.3499,  ..., 0.4654, 0.4665, 0.4677],\n",
       "        [0.0293, 0.0354, 0.0407,  ..., 0.0768, 0.0777, 0.0784],\n",
       "        ...,\n",
       "        [0.0302, 0.0419, 0.0469,  ..., 0.0732, 0.0743, 0.0753],\n",
       "        [0.0993, 0.1422, 0.1829,  ..., 0.2430, 0.2442, 0.2453],\n",
       "        [0.0087, 0.0121, 0.0139,  ..., 0.0290, 0.0296, 0.0301]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.cumsum(a, dim=1)\n",
    "b = b[:, :20]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polar-trial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1322, 0.4677, 0.0784, 0.0295, 0.0293, 0.2295, 0.0854, 0.2857, 0.0627,\n",
       "        0.0331, 0.1492, 0.0641, 0.1287, 0.0796, 0.4730, 0.6499, 0.0410, 0.0866,\n",
       "        0.0783, 0.2417, 0.3833, 0.0112, 0.2029, 0.1008, 0.2673, 0.0663, 0.3556,\n",
       "        0.0649, 0.1541, 0.0321, 0.2197, 0.1043, 0.0825, 0.1649, 0.2421, 0.4685,\n",
       "        0.1464, 0.0382, 0.0175, 0.0996, 0.0560, 0.8657, 0.0472, 0.0218, 0.1519,\n",
       "        0.0501, 0.1703, 0.0994, 0.6367, 0.0200, 0.2369, 0.5181, 0.0210, 0.0337,\n",
       "        0.4883, 0.1757, 0.2170, 0.2282, 0.0355, 0.0102, 0.6769, 0.0340, 0.0835,\n",
       "        0.2154, 0.0489, 0.2493, 0.0576, 0.6750, 0.0360, 0.0959, 0.0621, 0.0393,\n",
       "        0.8953, 0.0298, 0.0703, 0.0521, 0.0565, 0.6365, 0.2314, 0.0309, 0.0192,\n",
       "        0.1491, 0.0337, 0.1330, 0.0429, 0.0612, 0.9981, 0.0691, 0.3158, 0.0754,\n",
       "        0.1059, 0.0522, 0.3490, 0.0489, 0.2075, 0.0346, 0.6829, 0.0753, 0.2453,\n",
       "        0.0301], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "developmental-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug['check_in_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug['total_sample_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-alaska",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
