{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional, Tuple, Union, Any, collections\n",
    "import torch\n",
    "import copy \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    WEIGHTS_NAME,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_distributed_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import math\n",
    "import copy \n",
    "import random\n",
    "from packaging import version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainerState, TrainingArguments\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "champion-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "MAX_LEN = 128\n",
    "CAN_NUM = 20\n",
    "num_of_rerank = 30\n",
    "\n",
    "# some parameters I cooked up that work reasonably well\n",
    "epochs = 1\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "SAVE_PATH = \"/mnt/nfs/work1/llcao/zonghaiyao/LM/\"\n",
    "\n",
    "global debug\n",
    "debug = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wicked-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki2021_GPT2Dataset(Dataset):\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates(GPT2LMHeadModel):\n",
    "    def __init__(self, config, MAX_LEN=None, CAN_NUM=None, num_of_rerank=None):\n",
    "        super().__init__(config)\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.CAN_NUM = CAN_NUM\n",
    "        self.num_of_rerank = num_of_rerank\n",
    "        self.VOCAB_SIZE = config.vocab_size\n",
    "        \n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.rerank_linear_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        labels=None,\n",
    "        is_training=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        # make some model parameter not change during rerank (like dropout) ??????????????\n",
    "        # model.eval()\n",
    "        \n",
    "        global debug\n",
    "\n",
    "        if is_training:\n",
    "            self.transformer.eval()\n",
    "\n",
    "        rerank_places = random.sample(np.arange(1, self.MAX_LEN).tolist(), k=self.num_of_rerank) #no duplicate\n",
    "        rerank_places = np.concatenate(([0], np.sort(rerank_places), [self.MAX_LEN])) #add first and last tokens to make segments\n",
    "        \n",
    "        past_key_values = None\n",
    "        all_rerank_hidden_states = []\n",
    "        all_rerank_labels = []\n",
    "        no_rerank_logits = []\n",
    "        check_out_num = 0\n",
    "        \n",
    "        if not is_training:\n",
    "            all_candidate_token_ids = []\n",
    "            all_input_ids = []\n",
    "            all_prediction_ids = []\n",
    "        \n",
    "        for i in range(self.num_of_rerank+1):\n",
    "            #normal stage\n",
    "            segment_input_ids = input_ids[:, rerank_places[i]:rerank_places[i+1]]\n",
    "\n",
    "            segment_outputs = self.transformer(\n",
    "                segment_input_ids,\n",
    "                past_key_values = past_key_values\n",
    "            )\n",
    "\n",
    "            segment_hidden = segment_outputs[0]\n",
    "            past_key_values = segment_outputs[1]\n",
    "\n",
    "            #rerank stage (just for rerank places)\n",
    "            if i == self.num_of_rerank:\n",
    "                break\n",
    "\n",
    "            #rerank stage\n",
    "            #get logits in rerank place\n",
    "            logits_before_rerank = self.lm_head(segment_hidden[:, -1, :])\n",
    "            #get candidate token ids according to the logits\n",
    "            candidate_token_logits, candidate_token_ids = torch.topk(logits_before_rerank, self.CAN_NUM)\n",
    "            rerank_labels = labels[..., rerank_places[i+1]]\n",
    "            \n",
    "            #check whether or not label in candidates\n",
    "            check_labels = rerank_labels.tolist()\n",
    "            check_candidates = candidate_token_ids.tolist()\n",
    "            \n",
    "            assert len(check_labels)==len(check_candidates)\n",
    "            \n",
    "            #check whether or not label in candidates, if not we do not do rerank\n",
    "            rerank_labels = []\n",
    "            check_in_index = []\n",
    "\n",
    "            for j in range(len(check_labels)): \n",
    "                if check_labels[j] in check_candidates[j]:\n",
    "                    rerank_labels.append(check_candidates[j].index(check_labels[j]))\n",
    "                    check_in_index.append(j)\n",
    "                else:\n",
    "                    check_out_num+=1\n",
    "            rerank_labels = torch.tensor(rerank_labels, device=input_ids.device)\n",
    "\n",
    "            if rerank_labels.shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                all_rerank_labels.append(rerank_labels)\n",
    "\n",
    "            #make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "            sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=input_ids.device) * 50256\n",
    "            candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "\n",
    "            \n",
    "            #get output from gpt2\n",
    "            rerank_outputs = self.transformer(candidate_context_ids,\n",
    "                            past_key_values=past_key_values,\n",
    "                          )\n",
    "\n",
    "            #get rerank logits for candidates\n",
    "            rerank_hidden_states = rerank_outputs[0][:, 2+self.CAN_NUM:2+self.CAN_NUM*2]\n",
    "\n",
    "            all_rerank_hidden_states.append(rerank_hidden_states[check_in_index])\n",
    "            no_rerank_logits.append(candidate_token_logits[check_in_index])\n",
    "            \n",
    "            if not is_training:\n",
    "                all_candidate_token_ids.append(candidate_token_ids[check_in_index])\n",
    "                all_prediction_ids.append(input_ids[:, rerank_places[i+1]][check_in_index])\n",
    "                all_input_ids.append(input_ids[:, :rerank_places[i+1]][check_in_index])\n",
    "\n",
    "        \n",
    "#         print(\"\\n batch info:\")\n",
    "#         print(\"there are \", check_out_num/(self.num_of_rerank*batch_size), \"labels not in candidates\")\n",
    "    \n",
    "        if is_training:\n",
    "            #model.train()\n",
    "            self.transformer.train()\n",
    "        \n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, self.CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "\n",
    "\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, self.CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "\n",
    "\n",
    "            return {\"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"rerank_loss\": rerank_loss,}\n",
    "        # for evaluation, we will evaluate the model's performance on different difficult level\n",
    "        else:\n",
    "            #-------------------------------------------------------------------------\n",
    "            # cal loss, loss = normal loss + rerank loss\n",
    "            loss_fct = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "            # cal rerank loss\n",
    "            rerank_loss = None\n",
    "\n",
    "            all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "            all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "            all_rerank_logits = torch.reshape(all_rerank_logits, [-1, self.CAN_NUM])\n",
    "            all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "\n",
    "            rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "\n",
    "            # cal normal loss in rerank place (for comparision with rerank results), only evaluate\n",
    "            normal_loss_in_rerank_place = None\n",
    "            no_rerank_logits = torch.cat(no_rerank_logits, 0)\n",
    "            no_rerank_logits = torch.reshape(no_rerank_logits, [-1, self.CAN_NUM])\n",
    "            #no_rerank_labels = torch.cat(all_rerank_labels, 0) #no_rerank_labels == all_rerank_labels\n",
    "\n",
    "            normal_loss_in_rerank_place = loss_fct(no_rerank_logits, all_rerank_labels)\n",
    "            \n",
    "            for i in range(len(all_input_ids)):\n",
    "                target = torch.ones(size = [all_input_ids[i].shape[0], self.MAX_LEN], dtype = torch.long, device=input_ids.device) * 50256\n",
    "                target[:, :all_input_ids[i].shape[1]] = all_input_ids[i]\n",
    "                all_input_ids[i] = target\n",
    "                \n",
    "            all_input_ids = torch.cat(all_input_ids, 0)\n",
    "            all_prediction_ids = torch.cat(all_prediction_ids, 0)\n",
    "            all_candidate_token_ids = torch.cat(all_candidate_token_ids, 0)\n",
    "\n",
    "            return {\"all_rerank_logits\": all_rerank_logits,\n",
    "                    \"no_rerank_logits\": no_rerank_logits,\n",
    "                    \"difficult_level\": all_rerank_labels,\n",
    "                    \"rerank_loss\": rerank_loss,\n",
    "                    \"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                    \"all_candidate_token_ids\": all_candidate_token_ids,\n",
    "                    \"all_prediction_ids\": all_prediction_ids,\n",
    "                    \"all_input_ids\": all_input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "\n",
    "# instantiate the model\n",
    "B_rerank_linear_head = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\n",
    "    \"/mnt/nfs/scratch1/weihaotan/randomize_candidates_order/40000\",\n",
    "    config=configuration).state_dict()['rerank_linear_head.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "published-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates were not initialized from the model checkpoint at gpt2 and are newly initialized: ['rerank_linear_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#\"results/baseline_wiki2021/exclude_cases_label_not_in_candidates_canNUM20/120000\"\n",
    "#\"/mnt/nfs/scratch1/weihaotan/randomize_candidates_order/100000\"\n",
    "A_model = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\"gpt2\", \n",
    "                                                                                    config=configuration,\n",
    "                                                                                    MAX_LEN = MAX_LEN,\n",
    "                                                                                    CAN_NUM = CAN_NUM, \n",
    "                                                                                    num_of_rerank = num_of_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impressive-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = A_model.state_dict()\n",
    "model_dict.update({'rerank_linear_head.weight': B_rerank_linear_head})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "native-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates.from_pretrained(\"gpt2\",\n",
    "                                                                                    state_dict = model_dict,\n",
    "                                                                                    config=configuration,\n",
    "                                                                                    MAX_LEN = MAX_LEN,\n",
    "                                                                                    CAN_NUM = CAN_NUM, \n",
    "                                                                                    num_of_rerank = num_of_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "empty-campbell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): rerankGPT2LMHeadModel_exclude_cases_label_not_in_candidates(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    (rerank_linear_head): Linear(in_features=768, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = torch.nn.DataParallel(model) # Encapsulate the model\n",
    "\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weird-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_train_dataset.pkl', 'rb') as f:\n",
    "#     train_input_ids = pickle.load(f)\n",
    "with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_validation_dataset.pkl', 'rb') as f:\n",
    "    validation_input_ids = pickle.load(f)\n",
    "with open(SAVE_PATH + 'data/wiki2021/wiki2021_0to4_inside_validation_dataset.pkl', 'rb') as f:\n",
    "    inside_validation_input_ids = pickle.load(f)\n",
    "    \n",
    "# train_dataset = wiki2021_GPT2Dataset(train_input_ids)\n",
    "validation_dataset = wiki2021_GPT2Dataset(validation_input_ids)\n",
    "inside_validation_dataset = wiki2021_GPT2Dataset(inside_validation_input_ids)\n",
    "\n",
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "# train_dataloader = DataLoader(\n",
    "#             train_dataset,  # The training samples.\n",
    "#             sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "#         )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# For inside_validation the order doesn't matter, so we'll just read them sequentially.\n",
    "inside_validation_dataloader = DataLoader(\n",
    "            inside_validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(inside_validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gross-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_eval_loss = 0\n",
    "total_eval_normal_loss = 0\n",
    "total_eval_rerank_loss = 0\n",
    "\n",
    "all_evaluate_rerank_logits = []\n",
    "all_evaluate_normal_logits = []\n",
    "all_evaluate_difficult_level = []\n",
    "all_evaluate_rerank_loss = []\n",
    "all_evaluate_normal_loss_in_rerank_place = []\n",
    "all_evaluate_ground_true = []\n",
    "all_evaluate_inputs_text = []\n",
    "all_evaluate_candidate_token = []\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in inside_validation_dataloader:        \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(input_ids=batch,         #batch_input_ids\n",
    "                        labels=batch,            #batch_labels\n",
    "                        is_training=False,\n",
    "                        )\n",
    "\n",
    "        normal_loss = outputs[\"normal_loss_in_rerank_place\"].mean()\n",
    "        rerank_loss = outputs[\"rerank_loss\"].mean()\n",
    "\n",
    "        loss = normal_loss + rerank_loss\n",
    "\n",
    "    batch_loss = loss.item()\n",
    "    total_eval_loss += batch_loss        \n",
    "\n",
    "    batch_normal_loss = normal_loss.item()\n",
    "    total_eval_normal_loss += batch_normal_loss\n",
    "\n",
    "    batch_rerank_loss = rerank_loss.item()\n",
    "    total_eval_rerank_loss += batch_rerank_loss\n",
    "\n",
    "    #fine-grained evaluation\n",
    "    all_evaluate_rerank_logits.extend(outputs[\"all_rerank_logits\"].tolist())\n",
    "    all_evaluate_normal_logits.extend(outputs[\"no_rerank_logits\"].tolist())\n",
    "    all_evaluate_difficult_level.extend(outputs[\"difficult_level\"].tolist())\n",
    "    all_evaluate_rerank_loss.extend(outputs[\"rerank_loss\"].tolist())\n",
    "    all_evaluate_normal_loss_in_rerank_place.extend(outputs[\"normal_loss_in_rerank_place\"].tolist())    \n",
    "    all_evaluate_ground_true.extend(tokenizer.batch_decode(outputs['all_prediction_ids'], skip_special_tokens=True))\n",
    "    all_evaluate_inputs_text.extend(tokenizer.batch_decode(outputs['all_input_ids'], skip_special_tokens=True))\n",
    "    all_evaluate_candidate_token.extend([tokenizer.batch_decode(ids) for ids in outputs['all_candidate_token_ids']])\n",
    "\n",
    "\n",
    "avg_val_loss = total_eval_loss / len(inside_validation_dataloader)\n",
    "avg_val_normal_loss = total_eval_normal_loss / len(inside_validation_dataloader)       \n",
    "avg_val_rerank_loss = total_eval_rerank_loss / len(inside_validation_dataloader)    \n",
    "\n",
    "validation_time = format_time(time.time() - t1)    \n",
    "\n",
    "print(\"  inside Validation Loss:\", avg_val_loss)\n",
    "print(\"  Average inside Validation normal_loss:\", avg_val_normal_loss)\n",
    "print(\"  Average inside Validation rerank_loss:\", avg_val_rerank_loss)\n",
    "print(\"  inside Validation took:\", validation_time)\n",
    "\n",
    "#fine_grained_evaluation\n",
    "fg_eval = pd.DataFrame({\"rerank_logits\": all_evaluate_rerank_logits,\n",
    "                        \"normal_logits\": all_evaluate_normal_logits,\n",
    "                        \"ground_true_difficulty_level\": all_evaluate_difficult_level,\n",
    "                        \"rerank_loss\": all_evaluate_rerank_loss,\n",
    "                        \"normal_loss\": all_evaluate_normal_loss_in_rerank_place,\n",
    "                        \"ground_true\": all_evaluate_ground_true,\n",
    "                        \"inputs_text\": all_evaluate_inputs_text,\n",
    "                        \"candidate_tokens\": all_evaluate_candidate_token,\n",
    "                        }) \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def cal_entropy_difficulty_level(x):\n",
    "    x = softmax(x)\n",
    "    entropy_difficulty_level = 0\n",
    "    for i in range(CAN_NUM):\n",
    "        entropy_difficulty_level -= x[i] * np.log10(x[i])\n",
    "\n",
    "    return entropy_difficulty_level\n",
    "\n",
    "fg_eval['entropy_difficulty_level'] = fg_eval['normal_logits'].apply(cal_entropy_difficulty_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-toyota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "isolated-violence",
   "metadata": {},
   "source": [
    "A_stage1 + B_stage2\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/10000\n",
    "    Average inside Validation normal_loss: 1.6714191806316376\n",
    "    Average inside Validation rerank_loss: 1.9366122102737426\n",
    "    \n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/10000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "    Average inside Validation normal_loss: 1.8418855500221252\n",
    "    Average inside Validation rerank_loss: 2.320475811958313\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "    Average inside Validation normal_loss: 1.6714191806316376\n",
    "    Average inside Validation rerank_loss: 1.6473224246501923\n",
    "    \n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "    Average inside Validation normal_loss: 1.6714191806316376\n",
    "    Average inside Validation rerank_loss: 1.6472831737995148\n",
    "    \n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "B: randomize_candidates_order_canNUM20/100000\n",
    "    Average inside Validation normal_loss: 1.6714191806316376\n",
    "    Average inside Validation rerank_loss: 1.9990103709697724\n",
    "\n",
    "A: randomize_candidates_order_canNUM20/10000\n",
    "B: randomize_candidates_order_canNUM20/100000\n",
    "    Average inside Validation normal_loss: 1.7695554792881012\n",
    "    Average inside Validation rerank_loss: 2.0903142416477203\n",
    "    \n",
    "A: randomize_candidates_order_canNUM20/90000\n",
    "B: randomize_candidates_order_canNUM20/90000\n",
    "    Average inside Validation normal_loss: 1.641195641899109\n",
    "    Average inside Validation rerank_loss: 1.6240006633758546\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "A: randomize_candidates_order_canNUM20/100000\n",
    "B: randomize_candidates_order_canNUM20/100000\n",
    "    Average inside Validation normal_loss: 1.6321956861019133\n",
    "    Average inside Validation rerank_loss: 1.617129691505432\n",
    "  \n",
    "A: randomize_candidates_order_canNUM20/100000\n",
    "B: randomize_candidates_order_canNUM20/90000\n",
    "    Average inside Validation normal_loss: 1.6321956861019133\n",
    "    Average inside Validation rerank_loss: 1.6144152915477752\n",
    "\n",
    "A: randomize_candidates_order_canNUM20/100000\n",
    "B: randomize_candidates_order_canNUM20/80000\n",
    "    Average inside Validation normal_loss: 1.6321956861019133\n",
    "    Average inside Validation rerank_loss: 1.616434761285782\n",
    "    \n",
    "A: randomize_candidates_order_canNUM20/100000\n",
    "B: randomize_candidates_order_canNUM20/70000\n",
    "    Average inside Validation normal_loss: 1.6321956861019133\n",
    "    Average inside Validation rerank_loss: 1.617782951593399\n",
    "    \n",
    "A: randomize_candidates_order_canNUM20/100000\n",
    "B: randomize_candidates_order_canNUM20/10000\n",
    "    Average inside Validation normal_loss: 1.6321956861019133\n",
    "    Average inside Validation rerank_loss: 1.8089317905902862\n",
    "--------------------------------------------------------------------\n",
    "A: randomize_candidates_order_canNUM20/10000\n",
    "B: randomize_candidates_order_canNUM20/100000\n",
    "    Average inside Validation normal_loss: 1.7695554792881012\n",
    "    Average inside Validation rerank_loss: 2.0903142416477203\n",
    "\n",
    "\n",
    "A: randomize_candidates_order_canNUM20/10000\n",
    "B: randomize_candidates_order_canNUM20/30000\n",
    "    Average inside Validation normal_loss: 1.7695554792881012\n",
    "    Average inside Validation rerank_loss: 1.8977360785007478\n",
    "\n",
    "    \n",
    "A: randomize_candidates_order_canNUM20/10000\n",
    "B: randomize_candidates_order_canNUM20/20000\n",
    "    Average inside Validation normal_loss: 1.7695554792881012\n",
    "    Average inside Validation rerank_loss: 1.8504534327983857\n",
    "\n",
    "    \n",
    "A: randomize_candidates_order_canNUM20/10000\n",
    "B: randomize_candidates_order_canNUM20/10000\n",
    "    Average inside Validation normal_loss: 1.7695554792881012\n",
    "    Average inside Validation rerank_loss: 1.8233012283325196\n",
    "--------------------------------------------------------------------\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/120000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.6576037967205048\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/115000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.6574534106254577\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.6575494277477265\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/105000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.6570714569091798\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/100000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.657228969335556\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/50000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.6772904920578002\n",
    "\n",
    "\n",
    "A: exclude_cases_label_not_in_candidates_canNUM20/110000\n",
    "B: exclude_cases_label_not_in_candidates_canNUM20/10000\n",
    "    Average inside Validation normal_loss: 1.6769459700584413\n",
    "    Average inside Validation rerank_loss: 1.9194261312484742\n",
    "--------------------------------------------------------------------\n",
    "A: gpt2\n",
    "B: randomize_candidates_order_canNUM20/10000\n",
    "    Average inside Validation normal_loss: 1.7388068437576294\n",
    "    Average inside Validation rerank_loss: 3.229707317352295\n",
    "\n",
    "  \n",
    "A: gpt2\n",
    "B: randomize_candidates_order_canNUM20/20000\n",
    "    Average inside Validation normal_loss: 1.7388068437576294\n",
    "    Average inside Validation rerank_loss: 3.4192676830291746\n",
    "\n",
    "\n",
    "A: gpt2\n",
    "B: randomize_candidates_order_canNUM20/30000\n",
    "    Average inside Validation normal_loss: 1.7388068437576294\n",
    "    Average inside Validation rerank_loss: 4.190405163764954\n",
    "\n",
    "    \n",
    "A: gpt2\n",
    "B: randomize_candidates_order_canNUM20/40000\n",
    "\n",
    "    \n",
    "A: gpt2\n",
    "B: randomize_candidates_order_canNUM20/50000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
