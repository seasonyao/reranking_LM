{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional, Tuple, Union, Any, collections\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import copy \n",
    "\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers import Trainer, TrainerState, TrainingArguments\n",
    "\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    WEIGHTS_NAME,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_distributed_available,\n",
    "    is_torch_tpu_available,\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "MAX_LEN = 128\n",
    "CAN_NUM = 5\n",
    "num_of_rerank = 3\n",
    "\n",
    "global debug\n",
    "debug = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "VOCAB_SIZE = configuration.vocab_size\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.rerank_linear_head = nn.Linear(config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        is_training=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        global debug\n",
    "        # make some model parameter not change during rerank (like dropout)\n",
    "        model.eval()\n",
    "        \n",
    "        debug['input_ids'] = input_ids\n",
    "        debug['attention_mask'] = attention_mask\n",
    "        debug['labels'] = labels\n",
    "\n",
    "        rerank_places = np.concatenate(([0], np.sort(np.random.randint(MAX_LEN-2-CAN_NUM*2, size = (num_of_rerank,))), [MAX_LEN]))\n",
    "        #prevent duplicate\n",
    "        while len(rerank_places) != len(np.unique(rerank_places)):\n",
    "            rerank_places = np.concatenate(([0], np.sort(np.random.randint(MAX_LEN-2-CAN_NUM*2, size = (num_of_rerank,))), [MAX_LEN]))\n",
    "        debug['rerank_places'] = rerank_places\n",
    "        \n",
    "        past_key_values = None\n",
    "        hidden_states = []\n",
    "        hidden_states_in_rerank_place = []\n",
    "        labels_in_rerank_place = []\n",
    "        all_rerank_hidden_states = []\n",
    "        all_rerank_labels = []\n",
    "        check_out_num = 0\n",
    "        for i in range(num_of_rerank+1):\n",
    "            #normal stage\n",
    "            segment_input_ids = input_ids[:, rerank_places[i]:rerank_places[i+1]]\n",
    "            #???attention_mask\n",
    "            #segment_attention_masks = attention_mask[:, rerank_places[i]:rerank_places[i+1]]\n",
    "            segment_attention_masks = attention_mask[:, :rerank_places[i+1]]\n",
    "            \n",
    "            debug['segment_input_ids'] = segment_input_ids\n",
    "            debug['segment_attention_masks'] = segment_attention_masks\n",
    "\n",
    "            #???attention_mask\n",
    "            segment_outputs = self.transformer(\n",
    "                segment_input_ids,\n",
    "                attention_mask = segment_attention_masks,\n",
    "                past_key_values = past_key_values\n",
    "            )\n",
    "            \n",
    "            debug['segment_outputs'] = segment_outputs\n",
    "\n",
    "            segment_hidden = segment_outputs[0]\n",
    "            past_key_values = segment_outputs[1]\n",
    "\n",
    "            hidden_states.append(segment_hidden)\n",
    "\n",
    "            #rerank stage (just for rerank places)\n",
    "            if i == num_of_rerank:\n",
    "                break\n",
    "\n",
    "            #rerank stage\n",
    "            #get logits in rerank place\n",
    "            logits_before_rerank = self.lm_head(segment_hidden[:, -1, :])\n",
    "            #get candidate token ids according to the logits\n",
    "            candidate_token_ids = torch.topk(logits_before_rerank, CAN_NUM).indices\n",
    "            rerank_labels = labels[..., rerank_places[i+1]]\n",
    "            labels_in_rerank_place.append(rerank_labels)\n",
    "            hidden_states_in_rerank_place.append(segment_hidden[:, -1, :])\n",
    "\n",
    "            debug['candidate_token_ids'] = candidate_token_ids\n",
    "            debug['rerank_labels'] = rerank_labels\n",
    "            \n",
    "            #check whether or not label in candidates\n",
    "            check_labels = rerank_labels.tolist()\n",
    "            check_candidates = candidate_token_ids.tolist()\n",
    "            \n",
    "            assert len(check_labels)==len(check_candidates)\n",
    "            \n",
    "            \n",
    "            #when training, check whether or not label in candidates, if not we add label into candidates\n",
    "            if is_training:\n",
    "                #check every data in batch\n",
    "                rerank_labels_this_place = []\n",
    "                for j in range(len(check_labels)):\n",
    "                    if check_labels[j] not in check_candidates[j]:\n",
    "                        check_out_num+=1\n",
    "                        replace_index = np.random.randint(CAN_NUM)\n",
    "                        candidate_token_ids[j][replace_index] = check_labels[j]\n",
    "                        rerank_labels_this_place.append(replace_index)          \n",
    "                    else:\n",
    "                        rerank_labels_this_place.append(check_candidates[j].index(check_labels[j]))\n",
    "                all_rerank_labels.append(torch.tensor(rerank_labels_this_place, device=device))\n",
    "            #when eval, check whether or not label in candidates, if not we do not do rerank\n",
    "            else:\n",
    "                rerank_labels = []\n",
    "                check_in_index = []\n",
    "\n",
    "                for j in range(len(check_labels)): \n",
    "                    if check_labels[j] in check_candidates[j]:\n",
    "                        rerank_labels.append(check_candidates[j].index(check_labels[j]))\n",
    "                        check_in_index.append(j)\n",
    "                    else:\n",
    "                        check_out_num+=1\n",
    "                rerank_labels = torch.tensor(rerank_labels, device=device)\n",
    "                    \n",
    "                if rerank_labels.shape[0] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    all_rerank_labels.append(rerank_labels)\n",
    "\n",
    "            #make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "            sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=device) * 50256\n",
    "            candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "\n",
    "            #get output from gpt2\n",
    "            rerank_outputs = self.transformer(candidate_context_ids,\n",
    "                            past_key_values=past_key_values,\n",
    "                          )\n",
    "\n",
    "            #get rerank logits for candidates\n",
    "            rerank_hidden_states = rerank_outputs[0][:, 2+CAN_NUM:2+CAN_NUM*2]\n",
    "\n",
    "            if not is_training:\n",
    "                all_rerank_hidden_states.append(rerank_hidden_states[check_in_index])\n",
    "            else:\n",
    "                all_rerank_hidden_states.append(rerank_hidden_states)\n",
    "        \n",
    "#         print(\"\\n batch info:\")\n",
    "#         print(\"there are \", check_out_num/(num_of_rerank*batch_size), \"labels not in candidates\")\n",
    "        # cal loss, loss = normal loss + rerank loss\n",
    "        loss = None\n",
    "\n",
    "        # cal normal loss\n",
    "        normal_loss = None\n",
    "        \n",
    "        hidden_states = torch.cat(hidden_states, 1)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        normal_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        # cal rerank loss\n",
    "        rerank_loss = None\n",
    "        \n",
    "        all_rerank_hidden_states = torch.cat(all_rerank_hidden_states, 0)\n",
    "        all_rerank_logits = self.rerank_linear_head(all_rerank_hidden_states)\n",
    "        all_rerank_logits = torch.reshape(all_rerank_logits, [-1, CAN_NUM])\n",
    "        all_rerank_labels = torch.cat(all_rerank_labels, 0)\n",
    "        \n",
    "        rerank_loss = loss_fct(all_rerank_logits, all_rerank_labels)\n",
    "        \n",
    "        # cal normal loss in rerank place (for comparision with rerank results)        \n",
    "        normal_loss_in_rerank_place = None\n",
    "\n",
    "        hidden_states_in_rerank_place = torch.cat(hidden_states_in_rerank_place, 0)\n",
    "        lm_logits_in_rerank_place = self.lm_head(hidden_states_in_rerank_place)\n",
    "        lm_logits_in_rerank_place = torch.reshape(lm_logits_in_rerank_place, [-1, VOCAB_SIZE])\n",
    "        labels_in_rerank_place = torch.cat(labels_in_rerank_place, 0)\n",
    "        \n",
    "        normal_loss_in_rerank_place = loss_fct(lm_logits_in_rerank_place, labels_in_rerank_place)\n",
    "        \n",
    "        #not sure which one to be used, loss is used to cal backward, others is for observation\n",
    "        #loss = rerank_loss\n",
    "        loss = normal_loss + rerank_loss\n",
    "        \n",
    "        #just output\n",
    "        #could not compare this two results when training, since rerank add correct labels to candidates\n",
    "#         print(\"normal_loss_in_rerank_place\", normal_loss_in_rerank_place)\n",
    "#         print(\"rerank_loss\", rerank_loss)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        return {\"loss\": loss,\n",
    "                \"normal_loss\": normal_loss,\n",
    "                \"normal_loss_in_rerank_place\": normal_loss_in_rerank_place,\n",
    "                \"rerank_loss\": rerank_loss,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is in \"/mnt/nfs/work1/llcao/zonghaiyao/LM/data/wikitext-2/my_train.csv\"\n",
    "train_df = pd.read_csv(\"data/wikitext-2/my_train.csv\")\n",
    "validation_df = pd.read_csv(\"data/wikitext-2/my_validation.csv\")\n",
    "test_df = pd.read_csv(\"data/wikitext-2/my_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|endoftext|> token has the id 50256\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|endoftext|> has the id 50256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of myGPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['rerank_linear_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT tokenizer.\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>') #gpt2-medium\n",
    "\n",
    "\n",
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))\n",
    "\n",
    "# instantiate the model\n",
    "model = myGPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "    \n",
    "        for txt in txt_list:\n",
    "            #encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            encodings_dict = tokenizer('<|endoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            debug['encodings_dict'] = encodings_dict\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GPT2Dataset(train_df['text'], tokenizer, max_length=MAX_LEN)\n",
    "validation_dataset = GPT2Dataset(validation_df['text'], tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = GPT2Dataset(test_df['text'], tokenizer, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50256, 373, 10810, 355, 262, 11695, 1266, 2646, 287, 262, 5701, 12121, 13, 220, 198, 383, 42941, 1754, 318, 18141, 351, 41525, 257, 39330, 287, 262, 11533, 286, 5933, 287, 262, 1578, 1829, 11, 543, 550, 587, 319, 262, 7794, 329, 4647, 13, 383, 2646, 635, 3181, 9465, 284, 28623, 5826, 45774, 11, 508, 11, 3805, 1719, 1839, 3294, 995, 27459, 11, 373, 9826, 6439, 284, 262, 2276, 1171, 13, 8673, 262, 6000, 33867, 286, 262, 2646, 338, 11533, 373, 257, 1103, 2488, 12, 31, 1204, 5933, 33376, 1754, 3706, 17421, 4024, 49367, 505, 13, 5826, 45774, 4752, 287, 281, 2720, 379, 262, 640, 286, 262, 2646, 338, 2650, 326, 262, 2095, 286, 8919, 376, 1381, 373, 1912, 319, 49367, 505, 11, 508, 379, 262, 640, 373, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug['encodings_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# For test the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 5.05\n",
      "  Average training normal_loss: 3.77\n",
      "  could not compare this two results when training, since rerank add correct labels to candidates\n",
      "  Average training normal_loss_in_rerank_place: 3.80\n",
      "  Average training rerank_loss: 1.28\n",
      "  Training epoch took: 0:29:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 5.21\n",
      "  Average Validation normal_loss: 3.94\n",
      "  not sure whether or not could compare this two results when evaluating. I only keep those cases where correct labels are in their candidates\n",
      "  Average Validation normal_loss_in_rerank_place: 3.95\n",
      "  Average Validation rerank_loss: 1.27\n",
      "  Validation took: 0:00:55\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 4.17\n",
      "  Average training normal_loss: 2.98\n",
      "  could not compare this two results when training, since rerank add correct labels to candidates\n",
      "  Average training normal_loss_in_rerank_place: 3.02\n",
      "  Average training rerank_loss: 1.19\n",
      "  Training epoch took: 0:28:55\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 5.06\n",
      "  Average Validation normal_loss: 3.90\n",
      "  not sure whether or not could compare this two results when evaluating. I only keep those cases where correct labels are in their candidates\n",
      "  Average Validation normal_loss_in_rerank_place: 3.91\n",
      "  Average Validation rerank_loss: 1.16\n",
      "  Validation took: 0:00:55\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 3.19\n",
      "  Average training normal_loss: 2.13\n",
      "  could not compare this two results when training, since rerank add correct labels to candidates\n",
      "  Average training normal_loss_in_rerank_place: 2.16\n",
      "  Average training rerank_loss: 1.05\n",
      "  Training epoch took: 0:29:09\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 5.43\n",
      "  Average Validation normal_loss: 4.26\n",
      "  not sure whether or not could compare this two results when evaluating. I only keep those cases where correct labels are in their candidates\n",
      "  Average Validation normal_loss_in_rerank_place: 4.33\n",
      "  Average Validation rerank_loss: 1.16\n",
      "  Validation took: 0:00:56\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:30:19 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_train_normal_loss = 0\n",
    "    total_train_normal_loss_in_rerank_place = 0\n",
    "    total_train_rerank_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, \n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None,\n",
    "                          is_training=True,\n",
    "                        )\n",
    "\n",
    "        debug['outputs'] = outputs\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        normal_loss = outputs[\"normal_loss\"]\n",
    "        normal_loss_in_rerank_place = outputs[\"normal_loss_in_rerank_place\"]\n",
    "        rerank_loss = outputs[\"rerank_loss\"]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        \n",
    "        batch_normal_loss = normal_loss.item()\n",
    "        total_train_normal_loss += batch_normal_loss\n",
    "        \n",
    "        #just output\n",
    "        #could not compare this two results when training, since rerank add correct labels to candidates\n",
    "        batch_normal_loss_in_rerank_place = normal_loss_in_rerank_place.item()\n",
    "        total_train_normal_loss_in_rerank_place += batch_normal_loss_in_rerank_place\n",
    "        \n",
    "        batch_rerank_loss = rerank_loss.item()\n",
    "        total_train_rerank_loss += batch_rerank_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    avg_train_normal_loss = total_train_normal_loss / len(train_dataloader)       \n",
    "    avg_train_normal_loss_in_rerank_place = total_train_normal_loss_in_rerank_place / len(train_dataloader)       \n",
    "    avg_train_rerank_loss = total_train_rerank_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Average training normal_loss: {0:.2f}\".format(avg_train_normal_loss))\n",
    "    print(\"  could not compare this two results when training, since rerank add correct labels to candidates\")\n",
    "    print(\"  Average training normal_loss_in_rerank_place: {0:.2f}\".format(avg_train_normal_loss_in_rerank_place))\n",
    "    print(\"  Average training rerank_loss: {0:.2f}\".format(avg_train_rerank_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    total_eval_normal_loss = 0\n",
    "    total_eval_normal_loss_in_rerank_place = 0\n",
    "    total_eval_rerank_loss = 0\n",
    "\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                             labels=b_labels,\n",
    "                             is_training=False,)\n",
    "          \n",
    "            loss = outputs[\"loss\"]\n",
    "            normal_loss = outputs[\"normal_loss\"]\n",
    "            normal_loss_in_rerank_place = outputs[\"normal_loss_in_rerank_place\"]\n",
    "            rerank_loss = outputs[\"rerank_loss\"]\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "        \n",
    "        batch_normal_loss = normal_loss.item()\n",
    "        total_eval_normal_loss += batch_normal_loss\n",
    "        \n",
    "        #not sure whether or not could compare this two results when evaluating\n",
    "        #I only keep those cases where correct labels are in their candidates\n",
    "        batch_normal_loss_in_rerank_place = normal_loss_in_rerank_place.item()\n",
    "        total_eval_normal_loss_in_rerank_place += batch_normal_loss_in_rerank_place\n",
    "        \n",
    "        batch_rerank_loss = rerank_loss.item()\n",
    "        total_eval_rerank_loss += batch_rerank_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    avg_val_normal_loss = total_eval_normal_loss / len(validation_dataloader)       \n",
    "    avg_val_normal_loss_in_rerank_place = total_eval_normal_loss_in_rerank_place / len(validation_dataloader)       \n",
    "    avg_val_rerank_loss = total_eval_rerank_loss / len(validation_dataloader)    \n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Average Validation normal_loss: {0:.2f}\".format(avg_val_normal_loss))\n",
    "    print(\"  not sure whether or not could compare this two results when evaluating. I only keep those cases where correct labels are in their candidates\")\n",
    "    print(\"  Average Validation normal_loss_in_rerank_place: {0:.2f}\".format(avg_val_normal_loss_in_rerank_place))\n",
    "    print(\"  Average Validation rerank_loss: {0:.2f}\".format(avg_val_rerank_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = None\n",
    "for batch in validation_dataloader:\n",
    "    eval_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> for holding or tearing the prey. Usually, the left claw is the crusher, and the right is the cutter. \\n The exoskeleton is generally blue above, with spots that coalesce, and yellow below. The red colour associated with lobsters only appears after cooking. This occurs because, in life, the red pigment astaxanthin is bound to a protein complex, but the complex is broken up by the heat of cooking, releasing the red pigment. \\n The closest relative of H. gammarus is the American lobster, Homarus americanus. The two species are very similar, and can be crossed'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(eval_batch[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,   329,  4769,   393, 24447,   262, 15974,    13, 19672,    11,\n",
       "           262,  1364, 26573,   318,   262, 25164,   372,    11,   290,   262,\n",
       "           826,   318,   262, 38121,    13,   220,   198,   383,   409,   418,\n",
       "         38800,   318,  4143,  4171,  2029,    11,   351, 10222,   326, 46064,\n",
       "           344,    11,   290,  7872,  2174,    13,   383,  2266,  9568,  3917,\n",
       "           351,  6804,  5937],\n",
       "        [50256,   329,  4769,   393, 24447,   262, 15974,    13, 19672,    11,\n",
       "           262,  1364, 26573,   318,   262, 25164,   372,    11,   290,   262,\n",
       "           826,   318,   262, 38121,    13,   220,   198,   383,   409,   418,\n",
       "         38800,   318,  4143,  4171,  2029,    11,   351, 10222,   326, 46064,\n",
       "           344,    11,   290,  7872,  2174,    13,   383,  2266,  9568,  3917,\n",
       "           351,  6804,  5937]], device='cuda:0')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "#48-->colour 50-->with\n",
    "predict_token_index = 53\n",
    "past_key_values=None\n",
    "#normal stage\n",
    "segment_input_ids = eval_batch[0][3][0:predict_token_index]\n",
    "labels = eval_batch[0][3][predict_token_index]\n",
    "segment_input_ids = torch.reshape(torch.cat([segment_input_ids, segment_input_ids], 0), [-1, predict_token_index])\n",
    "segment_input_ids = segment_input_ids.cuda()\n",
    "segment_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> for holding or tearing the prey. Usually, the left claw is the crusher, and the right is the cutter. \n",
      " The exoskeleton is generally blue above, with spots that coalesce, and yellow below. The red colour associated with lobsters only appears after cooking. This occurs because, in life, the red pigment astaxanthin is bound to a protein complex, but the complex is broken up by the heat of cooking, releasing the red pigment. \n",
      " The closest relative of H. gammarus is the American lobster, Homarus americanus. The two species are very similar, and can be crossed \n",
      "\n",
      "<|endoftext|> for holding or tearing the prey. Usually, the left claw is the crusher, and the right is the cutter. \n",
      " The exoskeleton is generally blue above, with spots that coalesce, and yellow below. The red colour associated with lobsters\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(eval_batch[0][3]), '\\n')\n",
    "print(tokenizer.decode(segment_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_outputs = model.transformer(\n",
    "    segment_input_ids,\n",
    "    past_key_values = past_key_values\n",
    ")\n",
    "\n",
    "segment_hidden = segment_outputs[0]\n",
    "past_key_values = segment_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 53, 768])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_before_rerank = model.lm_head(segment_hidden[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3876, 6.9071, 6.3623, 5.5878, 5.5738],\n",
       "        [8.3876, 6.9071, 6.3623, 5.5878, 5.5738]], device='cuda:0',\n",
       "       grad_fn=<TopkBackward>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(logits_before_rerank, CAN_NUM).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6756, 0.1537, 0.0891, 0.0411, 0.0405],\n",
       "        [0.6756, 0.1537, 0.0891, 0.0411, 0.0405]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(torch.topk(logits_before_rerank, CAN_NUM).values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is\n",
      " and\n",
      ",\n",
      " may\n",
      " gives\n"
     ]
    }
   ],
   "source": [
    "candidate_token_ids = torch.topk(logits_before_rerank, CAN_NUM).indices\n",
    "for i in range(CAN_NUM):\n",
    "    print(tokenizer.decode(candidate_token_ids[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' only'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_labels = labels\n",
    "tokenizer.decode(rerank_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> is and, may gives<|endoftext|> is and, may gives'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make context for rerank stage, 50256 is the token_id for </endoftext/>\n",
    "sep_token = torch.ones(size = [candidate_token_ids.shape[0], 1], dtype = torch.long, device=device) * 50256\n",
    "candidate_context_ids = torch.cat([sep_token, candidate_token_ids, sep_token, candidate_token_ids], -1)\n",
    "tokenizer.decode(candidate_context_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_outputs = model.transformer(candidate_context_ids,\n",
    "                past_key_values=past_key_values,\n",
    "              )\n",
    "\n",
    "rerank_hidden_states = rerank_outputs[0][:, 2+CAN_NUM:2+CAN_NUM*2]\n",
    "rerank_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2924],\n",
       "         [-0.4853],\n",
       "         [-0.9234],\n",
       "         [-0.9181],\n",
       "         [-0.1634]],\n",
       "\n",
       "        [[ 0.2924],\n",
       "         [-0.4853],\n",
       "         [-0.9234],\n",
       "         [-0.9181],\n",
       "         [-0.1634]]], device='cuda:0', grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rerank_linear_head(rerank_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3720],\n",
       "         [0.1709],\n",
       "         [0.1103],\n",
       "         [0.1109],\n",
       "         [0.2358]],\n",
       "\n",
       "        [[0.3720],\n",
       "         [0.1709],\n",
       "         [0.1103],\n",
       "         [0.1109],\n",
       "         [0.2358]]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(model.rerank_linear_head(rerank_hidden_states), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
